---
layout: single
title: "Dr. Zero: Self-Evolving Search Agents without Training Data 리뷰"
summary: "Dr. Zero: Self-Evolving Search Agents without Training Data 리뷰"
author: Kim Changhyeon
categories: papers
published: True
toc: True
toc_sticky: True
comments: True
---

## 1. 논문의 문제의식과 출발점

**Dr. Zero: Self-Evolving Search Agents without Training Data**는
정답(ground truth)이나 인간이 만든 학습 데이터 없이도 검색 기반 에이전트가 스스로 학습하고 진화할 수 있는지를 문제로 제기합니다.

기존의 검색 기반 QA나 에이전트 학습 방법은 대체로 정답이 명시된 감독 데이터, 인간 피드백(RLHF), 또는 명시적인 oracle에 의존해 왔습니다. 그러나 저자들은 이러한 전제들이 실제 환경에서는 비용이 높거나 성립하지 않는 경우가 많다는 점에 주목합니다. 특히 오픈 도메인 검색 환경에서는 정답 자체를 명확히 정의하기 어렵고, 정의할 수 있더라도 라벨링 비용이 과도하게 커진다는 문제가 반복적으로 나타납니다.

이 논문의 출발점은 명확합니다.
정답을 직접 제공하지 않더라도, **검색을 수행하는 행동 정책 자체는 학습될 수 있는가**라는 질문입니다. 따라서 이 연구는 지식의 정확성보다는, **정답이 없는 상황에서 학습이 가능한지**라는 근본적인 조건을 탐구합니다.

---

## 2. 문제 설정 및 핵심 개념

논문에서 설정한 문제는 **oracle-free search agent learning**입니다. 문제 설정의 핵심 제약은 다음과 같습니다.

* 정답 라벨이 존재하지 않습니다.
* 정답 여부를 판별하는 함수가 존재하지 않습니다.
* 외부 검색 환경만이 유일한 정보 소스입니다.

에이전트는 자연어 질문을 생성하고, 검색 엔진을 호출하며, 검색 결과를 바탕으로 추론과 답변을 생성합니다. 그러나 이 과정에서 답변이 맞았는지 틀렸는지를 직접 판단하는 신호는 제공되지 않습니다.

이를 대신하여 논문에서는 보상을 다음 요소들로 구성합니다.

* 검색 경로의 논리적 일관성
* 동일 질문 반복 시 결과의 재현성
* 불필요한 검색을 줄였는지에 대한 효율성

중요한 점은, 이 보상 함수가 **사실성(truthfulness)을 직접 포함하지 않는다는 점**입니다. 본 논문은 처음부터 truth-free reward setting을 명시적으로 채택합니다.

---

## 3. 제안 방법의 구조와 설계 의도

Dr. Zero의 핵심 구조는 **Proposer–Solver 이중 에이전트 구조**입니다.

* Proposer는 검색을 수행하며 질문을 생성합니다. 이때 너무 쉽지도, Solver가 전혀 풀 수 없을 정도로 어렵지도 않은 질문을 생성하는 것을 목표로 합니다.
* Solver는 Proposer가 생성한 질문을 입력으로 받아 동일한 검색 도구를 사용해 답변을 생성합니다.

두 에이전트는 동일한 LLM 백본에서 시작하며, 상호작용을 통해 함께 진화합니다. 이 구조의 설계 의도는 질문 생성과 문제 해결을 분리함으로써, 별도의 인간 개입 없이도 self-play 형태의 커리큘럼을 형성하는 데 있습니다.

Solver가 해결할 수 있는 문제 수준이 점차 높아지면, Proposer 역시 더 어려운 질문을 생성하게 되고, 이 과정이 반복되면서 전체 시스템이 단계적으로 진화합니다.

---

## 4. 학습 방식과 알고리즘적 선택

학습은 강화학습 기반으로 이루어집니다. 논문의 핵심 알고리즘적 선택은 **Hop-Grouped Relative Policy Optimization (HRPO)**입니다.

HRPO는 검색 경로에서 필요한 hop 수, 즉 검색 단계 수를 기준으로 질문을 그룹화한 뒤, 동일 hop 그룹 내에서 상대적인 성과를 비교하여 policy update를 수행합니다. 이 방식의 목적은 두 가지입니다.

첫째, 질문 난이도를 hop 기준으로 정렬함으로써 보상 분산을 줄이고, 비교 가능한 샘플만을 이용해 학습을 안정화합니다.
둘째, 너무 쉬운 문제나 지나치게 어려운 문제로 인해 학습이 붕괴되는 현상을 방지합니다.

이 과정에서도 여전히 정답 여부는 관측되지 않습니다. HRPO는 학습의 효율성과 안정성을 위한 선택이지, 사실성을 보장하기 위한 장치는 아닙니다.

---

## 5. 실험 설정과 평가 기준

실험은 오픈 도메인 검색 QA 환경에서 수행됩니다. 외부 검색 엔진을 사용하며, 표준 QA 벤치마크를 기반으로 기존 감독 학습 모델 및 search-based agent들과 비교합니다.

평가 지표는 다음을 중심으로 구성됩니다.

* 질문 해결 성공률
* 검색 효율성
* 반복 실행 시 결과 안정성

논문은 이러한 지표들이 **정책의 품질(policy quality)**을 평가하기 위한 지표임을 분명히 합니다.

---

## 6. 실험 결과 해석

실험 결과에서 Dr. Zero는 감독 학습 기반 모델과 유사하거나 일부 설정에서는 더 나은 성능을 보입니다. 

이 성능은 검색 경로 선택이 더 안정적으로 이루어졌고, 질문 난이도 조절이 효과적으로 작동했으며, self-play 구조가 발산하지 않고 수렴했음을 의미합니다. 논문은 이를 통해 oracle이 없는 환경에서도 search policy가 학습 가능하다는 점을 보여줍니다.

---

## 7. Ablation 및 추가 분석

논문은 hop 수 변화, Proposer–Solver 분리 여부, HRPO 적용 유무에 대한 ablation 분석을 제시합니다. 이 분석들은 공통적으로 하나의 가설을 검증합니다.

구조적 난이도 조절과 상대 비교 메커니즘이 없을 경우, self-evolving 학습은 쉽게 불안정해진다는 점입니다. 특히 hop 기반 그룹화를 제거하면 학습이 trivial한 정책으로 붕괴하거나 발산하는 현상이 관찰됩니다.

이는 Dr. Zero의 성능이 우연이 아니라 구조적 설계의 결과임을 뒷받침합니다.

---

## 8. 종합 정리 및 개인적 평가

이 논문은 정답이나 oracle이 전혀 없는 환경에서도 검색 기반 행동 정책은 self-play를 통해 안정적으로 학습될 수 있음을 보였습니다.

반면, 학습된 정책이 사실적으로 옳은지 판단할 방법은 존재하지 않으며, 환각은 구조적으로 배제되지 않는다는 한계는 명확하며, 결과는 검색 환경이 가진 편향을 그대로 내재합니다.

Dr. Zero는 지식 학습이나 진실 추론을 목표로 하지 않습니다. 이 논문은 정답이 없다는 전제를 받아들였을 때 학습이 어디까지 가능한지를 탐구한 연구로 이해하는 것이 가장 정확합니다.
