---
layout: single
title: "Data Augmentation for Deep Learning Regression Tasks by Machine Learning Models 리뷰"
summary: "Data Augmentation for Deep Learning Regression Tasks by Machine Learning Models 리뷰"
author: Kim Changhyeon
categories: papers
published: True
toc: True
toc_sticky: True
comments: True
---
### 1. 소개
- 딥러닝 모델은 기존의 Tabular Regression문제에서 ML을 여전히 잘 넘지 못함.
  - 왜냐면 DL은 더 많은 데이터가 필요하니까.
  - 그러면 Augmentation을 하면 되는 게 아닐까?

### 2.Related Work
- Data Augmentation   
  - 데이터 증강. 이미지를 돌리거나, 텍스트에서 어순을 바꾸는 등, Non-Tabular Data에서는 다양한 기법이 이미 나와있었는데, Tabular Data에는 이런 기법이 부족했음.
  - 기존에 나왔던 Mixup, ADA, VAE, HIST기반 기법등 기존에 나왔던 기법들도 효과가 제한적임.
- AutoDL
  - AutoKeras, H2O, Autogluon등 자동 DL 프레임우어크들이 나와버림.
  - 이걸 활용해서 Augmentation을 하면 되는게 아닐까?
 
### 3. Data Augmentation for Deep learning Regression using machine learning (DADR)
<img width="1643" height="528" alt="image" src="https://github.com/user-attachments/assets/199c4e24-de49-4312-9969-2b7b1e56042f" />
1. Train - Test로 나눈다
2. Train으로 AutoML을 학습시킨다
3. X_train과 noise를 더한 X_synth를 만들어, 2.에서 학습시킨 모델에 넣어 y를 예측하고 이를 y_synth로 잡는다.
4. train과 synth를 더해서 X,y combined를 만든다
5. 4의 combined를 가지고 AutoDL을 학습시킨다
6. 1에서 따로 빼 놓은 test로 결과를 평가한다. 이 때 Baseline은 1의 train만 가지고 학습시킨 AutoDL과 비교한다.

### 4. 실험 및 결과
- AutoML은 TPOT를 사용. 기본 매개변수로 10분간 학습.
- AutoDL은 AutoKeras, H2O, Autogluon 사용. H2O는 DL만 하게 제한, Autogluon은 최대 10분, H2O와 AutoKera는 200에포크로 제한.
- 20개의 데이터셋을 활용해 기존 증강 방법과 비교.

- 20개의 실제 데이터셋 대상으로
##### C-Mix / ADA / 순수 노이즈 추가 / 증강 없이 vs DADR로 비교 결과
| DS | H2O:Ours | H2O:ADA | H2O:C-mixup | H2O:Noise | H2O:Baseline | AutoKeras:Ours | AutoKeras:ADA | AutoKeras:C-mixup | AutoKeras:Noise | AutoKeras:Baseline | AutoGluon:Ours | AutoGluon:ADA | AutoGluon:C-mixup | AutoGluon:Noise | AutoGluon:Baseline |
|----|----------|---------|-------------|-----------|--------------|----------------|----------------|--------------------|------------------|---------------------|----------------|----------------|---------------------|------------------|----------------------|
| A  | 4495     | 6055    | 5824        | 5963      | 5545         | 4444           | 5686           | 9513               | 6044             | 6517                | 4547           | 4659           | 4654                | 4326             | 4640                 |
| B  | 0.61     | 0.69    | 0.70        | 0.67      | 0.74         | 0.64           | 0.84           | 0.73               | 0.84             | 0.73                | 0.61           | 0.65           | 0.67                | 0.69             | 0.64                 |
| C  | 1.1      | 1.6     | 1.9         | 1.9       | 1.5          | 1.2            | 1.8            | 2.0                | 1.8              | 1.6                 | 1.2            | 1.5            | 1.9                 | 1.8              | 1.6                  |
| D  | 6.3      | 9.6     | 11.5        | 9.0       | 8.6          | 6.5            | 10.9           | 12.4               | 9.8              | 8.2                 | 6.3            | 8.3            | 10.8                | 8.8              | 7.5                  |
| E  | 4.70     | 5.20    | 5.09        | 5.43      | 5.12         | 4.96           | 5.34           | 5.01               | 5.82             | 6.85                | 4.81           | 5.32           | 4.94                | 5.43             | 5.25                 |
| F  | 157      | 133     | 132         | 131       | 150          | 157            | 134            | 150                | 131              | 139                 | 159            | 142            | 130                 | 143              | 178                  |
| G  | 1.68     | 1.53    | 1.73        | 1.56      | 1.69         | 2.05           | 2.02           | 2.22               | 1.78             | 5.44                | 2.00           | 2.01           | 2.09                | 1.67             | 2.00                 |
| H  | 636      | 754     | 818         | 712       | 721          | 690            | 904            | 967                | 829              | 869                 | 661            | 790            | 775                 | 686              | 665                  |
| I  | 0.06     | 0.08    | 0.08        | 0.07      | 0.08         | 0.06           | 0.10           | 0.08               | 0.08             | 0.08                | 0.06           | 0.07           | 0.07                | 0.08             | 0.06                 |
| J  | 1.45     | 1.48    | 1.66        | 1.29      | 1.66         | 1.48           | 1.56           | 1.77               | 1.52             | 1.37                | 1.46           | 1.46           | 1.53                | 1.38             | 1.61                 |
| K  | 10.78    | 10.77   | 12.01       | 9.57      | 10.66        | 11.30          | 11.15          | 11.77              | 9.97             | 10.27               | 10.49          | 10.98          | 12.36               | 9.30             | 11.70                |
| L  | 1.72     | 1.30    | 1.91        | 1.48      | 1.69         | 1.69           | 1.49           | 2.30               | 1.71             | 1.37                | 1.69           | 1.62           | 1.53                | 2.15             | 1.75                 |
| M  | 1.54     | 1.55    | 1.59        | 1.80      | 1.62         | 1.57           | 1.54           | 1.58               | 1.74             | 2.20                | 1.54           | 1.68           | 1.53                | 1.71             | 1.51                 |
| N  | 4.61     | 2.97    | 2.57        | 4.89      | 4.37         | 4.56           | 3.07           | 2.95               | 4.98             | 3.83                | 4.67           | 3.47           | 2.98                | 5.74             | 5.23                 |
| O  | 0.04     | 0.02    | 0.02        | 0.02      | 0.02         | 0.04           | 0.02           | 0.02               | 0.03             | 0.04                | 0.03           | 0.03           | 0.04                | 0.02             | 0.02                 |
| P  | 6.73     | 4.38    | 4.09        | 5.30      | 5.96         | 8.46           | 5.25           | 5.43               | 5.26             | 6.85                | 4.20           | 4.10           | 4.55                | 4.78             | 4.14                 |
| Q  | 139.93   | 186.06  | 152.95      | 164.97    | 159.00       | 152.79         | 214.75         | 189.53             | 180.89           | 167.69              | 128.03         | 130.53         | 125.50              | 158.32           | 165.29               |
| R  | 2.24     | 3.08    | 3.45        | 2.96      | 3.17         | 2.60           | 3.68           | 3.82               | 3.31             | 3.47                | 2.46           | 3.18           | 3.55                | 3.15             | 3.35                 |
| S  | 0.13     | 0.09    | 0.16        | 0.18      | 0.09         | 0.13           | 0.08           | 0.14               | 0.21             | 0.10                | 0.13           | 0.10           | 0.14                | 0.13             | 0.21                 |
| T  | 0.30     | 0.32    | 0.51        | 0.48      | 0.26         | 0.31           | 0.30           | 0.40               | 0.54             | 0.33                | 0.37           | 0.37           | 0.34                | 0.48             | 0.43                 |
| # best | 10   | 2       | 3           | 3         | 2            | 9              | 4              | 2                  | 3                | 2                   | 7              | 2              | 5                   | 5                | 1                    |


- 전체 AutoML에 대해서 # best가 가장 높은 것을 확인할 수 있으며, 특히 H2O에서는 50%에 해당하는 케이스에서 BEST를 반환하는 것을 볼 수 있음. 

##### Train Size와 증강 Size에 따른 성능 비교 (행이 Train Size, 열이 Augment Size)
**H2O**
| Train size<br>Augment Size | 500           | 1,000         | 5,000         | 10,000        | 50,000 or 80% |
|------------|---------------|---------------|---------------|---------------|----------------|
| 500        | 5.03% (3.13%) | 3.34% (2.88%) | 0.80% (0.54%) | 0.05% (-0.14%)| 0.12% (0.08%)  |
| 1,000      | 8.77% (4.73%) | 4.68% (3.51%) | 1.41% (1.24%) | 0.50% (0.41%) | 0.35% (0.20%)  |
| 5,000      | 8.99% (6.93%) | 8.77% (6.09%) | 3.83% (2.83%) | 3.68% (3.30%) | 1.47% (1.32%)  |
| 10,000     | 7.76% (7.03%) | 6.80% (7.32%) | 5.53% (4.40%) | 5.20% (4.35%) | 2.49% (1.95%)  |
| 50,000     | 12.70% (12.43%)|13.87% (14.54%)|13.90% (11.93%)|10.99% (9.05%) | 2.68% (1.23%)  |

**AutoKeras**
| Train size<br>Augment Size | 500           | 1,000         | 5,000         | 10,000        | 50,000 or 80% |
|------------|---------------|---------------|---------------|---------------|----------------|
| 500        | 37.03% (5.31%)|10.69% (2.35%) | 3.11% (3.88%) |22.48% (6.90%) | 1.12% (0.10%)  |
| 1,000      | 21.66% (7.05%)|48.24% (6.55%) | 5.63% (4.69%) |37.52% (6.19%) | 1.78% (0.63%)  |
| 5,000      | 60.37% (18.08%)|39.74% (13.80%)| 8.71% (7.00%) |42.15% (8.71%) |35.54% (6.65%)  |
| 10,000     | 85.12% (20.32%)|63.28% (16.70%)| 7.82% (6.37%) |43.43% (6.53%) |33.56% (7.02%)  |
| 50,000     | 63.34% (23.14%)|40.03% (16.36%)| 9.24% (9.04%) |45.93% (7.39%) |29.95% (4.72%)  |

**Autogluon**
| Train size<br>Augment Size | 500           | 1,000         | 5,000         | 10,000        | 50,000 or 80% |
|------------|---------------|---------------|---------------|---------------|----------------|
| 500        | 8.48% (3.53%) | 5.57% (1.92%) | 3.15% (2.01%) | 2.23% (1.31%) | 3.21% (1.47%)  |
| 1,000      | 8.12% (3.13%) | 6.43% (2.79%) | 3.25% (2.29%) | 2.72% (1.56%) | 2.74% (0.80%)  |
| 5,000      |11.65% (6.05%) | 9.06% (3.30%) | 3.95% (1.56%) | 3.15% (2.12%) | 3.00% (1.59%)  |
| 10,000     | 9.58% (5.81%) | 9.01% (3.61%) | 4.18% (1.69%) | 3.13% (1.91%) | 2.75% (1.01%)  |
| 50,000     | 8.12% (4.40%) | 4.62% (3.25%) | 2.96% (2.05%) | 2.29% (1.93%) | 1.39% (-0.03%) |

- 이처럼 Train Size가 작을 수록, 증강을 더 만힝 할수록 증강의 효과는 더 커지는 것을 확인 가능. 
##### 증강 vs 증류
- 사실 지식 증류의 효과인거 아님?
- 어느 정도는 맞다고 볼 수 있음!
- 논문도 이부분에 대해서 설명이 부족함. 뭐 증강의 효과가 더 큰 걸 검증했다고 하는데, 일단 본인은 어케 검증했는지 못찾음.


##### 느낀 점
1. Tabular Data의 증강을 시도했다는 것 자체가 흥미로움. 실제로 업무를 하다보면 진짜 쥐꼬리만한 데이터로 자꾸 뭘 해야 하는 때가 찾아오고 그래서 찾아본 논문이기도 함.
2. ML을 가지고 DL을 학습시키는 접근이 아니더라도, ML to ML 형태의 증강도 가능하지 않나? 싶어서 Autogluon만 가지고 해봤는데 실제로 어느 정도 성과를 거뒀음. 앞으로도 종종 사용해볼듯.
3. 증류인지 증강인지 애매하게 끝내는 걸 보면 저자도 자신은 없는듯? 아무튼 성능이 올라갔으니 OK다 하고 넘어가기에는 좀 애매한 느낌이 있는 건 사실.
