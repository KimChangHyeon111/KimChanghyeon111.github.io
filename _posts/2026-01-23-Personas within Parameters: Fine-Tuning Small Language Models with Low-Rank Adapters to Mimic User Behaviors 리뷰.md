---
layout: single
title: "Personas within Parameters: Fine-Tuning Small Language Models with Low-Rank Adapters to Mimic User Behaviors"
summary: "Personas within Parameters: Fine-Tuning Small Language Models with Low-Rank Adapters to Mimic User Behaviors 논문 리뷰"
author: Kim Changhyeon
categories: papers
published: true
toc: true
toc_sticky: true
comments: true
---

# Personas within Parameters: Fine-Tuning Small Language Models with Low-Rank Adapters to Mimic User Behaviors

## 1. 논문의 문제의식과 출발점

이 논문은 추천 시스템에서 **사용자 행동 시뮬레이션을 어떻게 확장 가능하면서도 개인화 수준을 유지한 채 구현할 수 있는가**라는 문제에서 출발합니다. 기존 추천 모델은 로그 기반 예측 정확도는 높지만, 실제 서비스 환경에서의 사용자 반응과 괴리가 반복적으로 발생해 왔습니다. 이로 인해 오프라인 실험 결과가 실제 온라인 성능으로 이어지지 않는 문제가 지속적으로 제기되었습니다.

최근에는 LLM 기반 사용자 에이전트가 이러한 간극을 줄이기 위한 대안으로 제안되었으나, 논문은 LLM 중심 접근이 구조적으로 세 가지 한계를 가진다고 지적합니다. 첫째, 대규모 사용자 로그를 지속적으로 처리하기에는 비용이 과도합니다. 둘째, 사전학습 과정에서 형성된 inductive bias로 인해 특정 사용자의 고유한 선호를 정확히 내재화하기 어렵습니다. 셋째, 수백만 명의 사용자를 개별 모델로 다루는 것은 시스템적으로 불가능합니다.

이에 대해 논문은 **LLM은 사용자 이해를 위한 도구로 제한하고, 실제 사용자 행동을 모사하는 주체는 SLM으로 설계한다**는 명확한 역할 분리를 제안합니다. 즉, LLM은 해석과 증류에만 사용하고, 비용 효율적인 소형 언어모델(SLM)에 사용자 행동을 내재화함으로써 확장성과 실용성을 동시에 확보하는 것이 핵심 출발점입니다.

---

## 2. 문제 설정 및 핵심 개념

논문은 전통적인 추천 시스템 설정을 기반으로 사용자 에이전트를 정의합니다. 사용자 집합 U, 아이템 집합 I, 사용자–아이템 상호작용 기록 D가 주어질 때, 목표는 특정 사용자 u가 아이템 i에 대해 보일 반응 y를 예측하는 것입니다.

이때 논문이 도입하는 핵심 개념은 **단기 기억과 장기 기억을 갖는 사용자 에이전트**입니다.

- 단기 기억(Ms): 사용자의 비교적 안정적인 성향과 취향을 요약한 텍스트 프로필  
- 장기 기억(Ml): 과거 특정 아이템에 대해 내린 결정과 그 이유를 설명한 텍스트 기반 기억  

사용자 에이전트는 다음과 같은 형태로 정의됩니다.

- 입력: 사용자 단기 기억, 현재 아이템 설명, 장기 기억 중 일부
- 출력: 해당 아이템에 대한 사용자 반응(예: 평점)

중요한 제약은 SLM의 컨텍스트 길이가 제한되어 있다는 점입니다. 따라서 모든 과거 이력을 입력에 포함하는 것이 아니라, **현재 아이템과 유사한 일부 장기 기억만을 검색하여 사용**합니다. 이 선택적 기억 참조가 이후 RAFT 설계의 핵심 전제가 됩니다.

---

## 3. 제안 방법의 전체 구조

논문이 제안하는 방법은 세 단계로 구성됩니다.

### 3.1 사용자 페르소나 생성 (Stage 1)

첫 단계의 목적은 **표 형식의 사용자–아이템 상호작용 로그를 SLM이 이해 가능한 텍스트 지식으로 변환하는 것**입니다. 이를 위해 연구진은 GPT-4o를 고정된 상태로 사용하여, 사용자별 상호작용 이력을 시간 순서로 배치 단위(batch ≤ 10)로 입력합니다.

이 과정에서 단순 요약이 아닌 **계층적 자기 성찰(hierarchical self-reflection)**을 적용합니다. 즉, LLM이 “무엇을 선택했는가”가 아니라 “왜 그렇게 선택했는가”를 서술하도록 유도합니다. 그 결과 사용자마다 다음 두 가지 산출물이 생성됩니다.

- Ms: 활동성, 동조성, 다양성 추구 성향, 상세 취향을 포함한 사용자 프로필  
- Ml: 특정 아이템에 대해 좋아하거나 싫어한 이유를 설명한 강화된 상호작용 설명  

이 단계에서 LLM은 학습 대상이 아니라 **지식 추출기**로만 사용됩니다.

### 3.2 페르소나 기반 LoRA 미세 조정 (Stage 2)

모든 사용자마다 개별 LoRA를 학습하는 것은 비현실적이므로, 연구진은 Ms를 임베딩한 뒤 KMeans++로 사용자 클러스터링을 수행합니다. 실험에서는 엘보우 방법을 통해 페르소나 수를 K=4로 설정합니다.

각 페르소나 그룹에 대해 하나의 LoRA 어댑터를 학습하며, 베이스 SLM(Phi-3-Mini)의 가중치는 고정됩니다. 이 설계는 개인화 수준과 파라미터 효율성 간의 균형을 명확히 의도한 선택입니다.

### 3.3 사용자 에이전트 구성 (Stage 3)

추론 시에는 다음 요소들이 결합됩니다.

- 베이스 SLM + 해당 페르소나 LoRA  
- 사용자 고유의 단기 기억(Ms)을 시스템 프롬프트로 사용  
- 현재 아이템과 유사한 장기 기억(Ml)을 검색하여 입력에 추가  

이 구조를 통해 모델 파라미터 수는 제한하면서도, 사용자별 행동 차이를 프롬프트와 기억 수준에서 유지할 수 있습니다.

---

## 4. RAFT: Retrieval Augmented Fine-Tuning의 핵심 설계

이 논문의 중요한 차별점은 **RAG를 추론 단계뿐 아니라 학습 단계에도 사용한다는 점**입니다. 이를 논문에서는 Retrieval Augmented Fine-Tuning(RAFT)이라 부릅니다.

일반적인 미세 조정은 다음과 같은 구조를 가집니다.

- 입력: 사용자 프로필 + 현재 아이템  
- 출력: 실제 사용자 반응  

반면 RAFT에서는 학습 데이터 자체가 다음과 같이 구성됩니다.

- 입력:  
  - 사용자 단기 기억(Ms)  
  - 현재 아이템 설명  
  - 현재 아이템과 가장 유사한 과거 상호작용의 강화된 설명(Ml, k=1)  
- 출력: 실제 사용자가 부여한 평점  

즉, 모델은 학습 단계에서부터 **“과거에 비슷한 상황에서 왜 그런 결정을 내렸는지”를 함께 보고 현재 판단을 내리는 방식**을 학습합니다. 이는 단순히 기억을 참고하는 것이 아니라, **기억을 활용하는 사고 방식 자체를 LoRA 파라미터에 내재화**하는 과정으로 해석할 수 있습니다.

이 설계는 SLM이 갖는 장기 기억 한계를 보완합니다. 장기 기억 전체를 모델 파라미터에 저장하지 않고, 기억을 해석하는 규칙만을 학습시킴으로써 제한된 모델 용량에서도 개인화된 행동 모사가 가능해집니다.

---

## 5. 실험 설정과 평가 기준

실험은 MovieLens-1M 데이터셋을 기반으로 수행됩니다. 200명의 사용자를 선택하고, 사용자당 100~200개의 상호작용을 사용합니다. 데이터는 시간 기준 60:40으로 분할됩니다.

비교 대상은 다음과 같습니다.

- LLaMA-3-8B (프롬프트 기반)  
- Phi-3-Mini (프롬프트 기반)  
- Phi-3-Mini + 단일 LoRA  
- Phi-3-Mini + 페르소나별 LoRA  

평가 지표는 RMSE, MAE, 그리고 과제와 무관한 응답 비율을 측정하는 URR(Unrelated Response Rate)입니다.

---

## 6. 실험 결과 해석

실험 결과는 논문의 주장을 일관되게 뒷받침합니다.

첫째, **미세 조정된 SLM은 프롬프트 기반 LLM보다 우수하거나 동등한 성능**을 보입니다. 이는 모델 크기보다 사용자 지식을 어떻게 구조화해 내재화하느냐가 더 중요함을 시사합니다.

둘째, 단기 기억만 사용하는 경우보다 단기 기억과 장기 기억을 함께 사용하는 경우 대부분의 설정에서 성능이 개선됩니다. 이는 강화된 상호작용 설명이 실제 사용자 판단 논리를 효과적으로 보완함을 의미합니다.

셋째, 페르소나 기반 LoRA는 단일 LoRA 대비 데이터 효율성과 성능 간의 균형 측면에서 유리합니다. 다만 일부 페르소나에서는 데이터 부족으로 성능 저하가 관찰되며, 이는 클러스터 크기와 성능 간의 직접적인 연관성을 보여줍니다.

---

## 7. Ablation 및 추가 분석

Ablation 실험은 두 가지 가설을 검증합니다.

- 장기 기억(Ml)이 항상 도움이 되는가  
- 사용자 클러스터링이 실제로 의미 있는가  

결과적으로 장기 기억은 대체로 성능을 개선하지만, 검색 품질이 낮은 경우 오히려 일반화를 저해할 수 있음이 확인됩니다. 이는 RAFT 구조가 **검색 품질에 민감한 설계**임을 보여주며, 논문에서도 향후 연구 과제로 명시됩니다.

또한 페르소나별 성능은 학습 데이터 크기에 크게 의존하며, 이는 페르소나 설계가 단순한 군집 문제가 아니라 데이터 분배 문제와 직결됨을 시사합니다.

---

## 8. 종합 정리 및 개인적 평가

이 논문의 핵심 기여는 다음 한 문장으로 요약할 수 있습니다.

**“대규모 사용자 시뮬레이션은 거대 모델이 아니라, 지식을 잘 정제한 소형 모델로 구현할 수 있다.”**

- LLM을 사용자 이해를 위한 증류 도구로 한정하고, SLM + LoRA + 메모리 구조를 결합한 설계는 실무적으로 매우 현실적인 해법이라고 생각
- LoRA의 수를 소수로 한정하고, RAG와 결합해서 사용하는 접근도 현실적인 현업의 어프로치라고 생각
- 여전히 RAFT는 뭐가 특별한지 잘 모르겠음. 그냥...학습데이터...포맷을 다르게 만들었을 뿐인거 아닌가 싶고.
