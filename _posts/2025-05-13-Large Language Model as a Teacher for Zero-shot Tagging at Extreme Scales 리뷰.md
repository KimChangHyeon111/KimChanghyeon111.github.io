---
layout: posts
title: "Large Language Model as a Teacher for Zero-shot Tagging at Extreme Scales 리뷰"
summary: "Large Language Model as a Teacher for Zero-shot Tagging at Extreme Scales 리뷰"
author: Kim Changhyeon
categories: papers
published: True
toc: True
toc_sticky: True
comments: True
---
### 1. 소개
- Extreme Zero-shot Extreme Multi-label Text Classification (EZ-XMC) : 수천개의 라벨 중에 적절한 라벨을 찾아야 하는데, 레이블이 없는 문제. 다만 레이블의 설명은 존재하는 상황에 사용이 가능함.
- 본 논문은 LLM으로 고품질 pseudo-label을 생성하고, 추론시에는 경량 Bi-Encoder만 사용하는 LMTX 기법을 제안.
   - LMTX는 **더 적은 데이터를 필요**로 하는데, 애초에 pseudo-label과 설명 같에 높은 상관성이 있는 데이터를 가져와, 데이터의 퀄리티가 좋기 때문임.
   - LMTX는 **더 가벼운 모델**을 만드는데, bi-encoder만 사용하고 LLM이 연관되지 않기 때문.

### 2.문제 정의
- X_i : 문서. ex. 아마존의 상품 설명. 정답 라벨이 없다는게 EZ-XMC 문제의 특징.
- l_i : 라벨. X와 매핑이 되지는 않지만, 텍스트 설명은 존재함.
- E_θ : 문서 X에 대해 적절한 l을 찾는 것이 목적. 이를 위해 둘 다를 동일한 임베딩 공간(S)에 매핑하는 함수가 바로 E_θ. Bi-Encoder구조로 구현됨.

  **Bi-Encoder 구조**
  - 문서와 라벨을 각각 Transformers 기반 인코더에 넣어 임베딩
  - 두 인코더는 가중치를 공유
  - 이 때 문서와 라벨의 관련성은 두 임베딩간의 코사인 유사도로 계산하며, DistillBERT 기반으로 구성


### 3. Bi-Encoder 훈련
- **3단계의 반복적인 프레임워크**를 채택.
![image](https://github.com/user-attachments/assets/5916d23c-56fe-4fb5-a813-82be39927731)
**Bi-Encoder와 LLM의 피드백 수용, 학습 루틴 시각화**
  1. 모든 문서와 라벨을 인베딩한 후, 라벨 임베딩에 대한 근사 최근접 탐색을 통해 문서별 후보 라벨을 얻음 (Cos)
    - 그냥 LLM에 다 때려넣으면 안됨? -> 계산 복잡도도 높고, 비쌈
    - 그래서 Bi-Encoder를 이용해 임배딩 후, 근사 최근접 탐색을 통해 j개의 후보 라벨을 선택     
  2. LLM을 활용해 이 라벨과 문서에 대한 관련성 검토를 수행하고, 이를기반으로 pseudo-positive label을 식별한다
    - 이를 가지고 다음과 같은 질문으로 LLM에게 프롬프트로 제공
      ```ini
      document = {X_i}, is the tag {l_k} relevant to the document? answer yes or no
      ```
    - 최종적으로 yes를 받은 라벨 중 하나만을 활용해 Bi-Encoder훈련
      <span style="color:red">: 뭐로 고르는지 기준은 딱히 안나오는듯. 아마...유사도 기준으로 고르지 않을까?</span>

  3. 이를 가지고 Bi-Encoder를 훈련한다.
    - Triplet Loss를 손실함수로 사용:
      $$ L = \sum_{i=1}^{N}\; \sum_{k' \neq p} \max\!\Bigl(0,\, \langle E_{\theta}(X_{i}),\, E_{\theta}(l_{k'}) \rangle - \langle E_{\theta}(X_{i}),\, E_{\theta}(l_{p}) \rangle + \gamma \Bigr) $$
      : **"문서와 양성 라벨은 코사인 유사도가 높아야 하고, 문서와 다른 라벨(음성)은 유사도가 낮아야 한다. 그 차이가 γ보다 작으면 패널티를 준다."**

    - 앞서 LLM으로 뽑아낸 하나의 양성 라벨과 
    - 다른 문서의 라벨을 음성 라벨으로 채택해서 학습한다.
    - 예를 들어
      - 배치에서 문서 A는 충전기, 문서 B는 라면이라고 양성 라벨을 뽑았다면, 문서 A의 음성 라벨은 라면으로 지정한다는 것.
      - 논문에 따르면 진짜 LLM이 NO라고 한 라벨을 넣으면 오히려 성능이 떨어진다고 함. 

### 4. 실험
#### 사용된 데이터셋

| 데이터셋                   | 총 샘플 수 | 테스트 샘플 수 | 라벨 수     | 라벨 당 평균 샘플 수 |
|----------------------------|--------------|----------------|-------------|----------------|
| EURLex-4K                 | 15,511       | 3,803          | 3,956       | 20.79          |
| Wiki10-31K                | 14,146       | 6,616          | 30,938      | 8.52           |
| AmazonCat-13K             | 1,186,239    | 306,782        | 13,330      | 448.57         |
| LF-WikiSeeAlso-320K       | 693,082      | 177,515        | 312,330     | 4.67           |
| LF-Wikipedia-500K         | 1,813,391    | 783,743        | 501,070     | 24.75          |

- ⚠️ 대규모 데이터셋(AmazonCat, WikiSeeAlso, Wikipedia)에서는 학습 시간 단축을 위해 **30,000개 샘플만 사용**.  
- Baseline 모델은 전체 데이터셋 사용.

#### 모델 성능 비교 (Precision@1 기준)

| Dataset            | Method               | P@1   | P@3   | P@5   | R@1     | R@3   | R@5     | R@10  |
|--------------------|----------------------|-------|-------|-------|--------|--------|--------|--------|
| **EURLex-4K**      | Glove                | 1.66  | 1.11  | 1.04  | 0.37   | 0.73   | 1.08   | 1.88   |
|                    | SentBERT             | 8.52  | 7.70  | 6.83  | 1.70   | 4.54   | 6.69   | 10.20  |
|                    | SimCSE               | 5.86  | 4.44  | 3.85  | 1.20   | 2.86   | 3.93   | 6.12   |
|                    | MPNet                | 10.81 | 8.65  | 7.21  | 2.27   | 5.28   | 7.28   | 10.85  |
|                    | MSMARCO-distilBERT   | 15.91 | 9.89  | 7.81  | 3.33   | 6.16   | 8.08   | 11.22  |
|                    | RTS §                | 30.58 | 21.54 | 17.73 | 6.19   | 13.01  | 17.72  | 25.34  |
|                    | **LMTX †**           | **47.28** | **29.34** | **21.98** | **9.6** | **17.68** | **21.96** | **28.44** |
|                    |                      |       |       |       |        |        |        |        |
| **Wiki10-31K**     | Glove                | 3.87  | 3.11  | 2.87  | 0.24   | 0.57   | 0.89   | 1.48   |
|                    | SentBERT             | 9.39  | 6.93  | 5.81  | 0.60   | 1.31   | 1.81   | 2.70   |
|                    | SimCSE               | 23.55 | 17.21 | 14.01 | 1.42   | 3.07   | 4.13   | 6.01   |
|                    | MPNet                | 44.82 | 29.18 | 22.38 | 2.63   | 5.12   | 6.52   | 8.89   |
|                    | MSMARCO-distilBERT   | **54.17 §** | 33.44 | 25.38 | **3.18** | **5.82** | **7.32** | **9.70** |
|                    | RTS                  | 47.73 | 31.03 | 23.65 | 2.81   | 5.41   | 6.84   | 9.12   |
|                    | **LMTX †**           | **57.89** | **38.00** | **29.09** | **3.41** | **6.68** | **8.46** | **11.14** |
|                    |                      |       |       |       |        |        |        |        |
| **AmazonCat-13K**  | Glove                | 4.83  | 3.89  | 3.42  | 0.99   | 2.46   | 3.67   | 6.05   |
|                    | SentBERT             | 5.21  | 4.22  | 3.68  | 0.99   | 2.34   | 3.37   | 5.35   |
|                    | SimCSE               | 2.84  | 2.60  | 2.42  | 0.52   | 1.41   | 2.17   | 3.75   |
|                    | ICT                  | 15.52 | 10.48 | 8.34  | 2.91   | 5.93   | 7.86   | 11.04  |
|                    | MPNet                | 18.01 | 12.84 | 10.51 | 3.63   | 7.68   | 10.48  | 15.73  |
|                    | MACLR                | 10.66 | 6.75  | 5.14  | 1.98   | 3.79   | 4.81   | 6.35   |
|                    | MSMARCO-distilBERT   | 16.36 | 10.96 | 8.68  | 3.29   | 6.62   | 8.73   | 12.23  |
|                    | RTS §                | 18.89 | 13.59 | 11.07 | 3.69   | 8.03   | 10.97  | 16.20  |
|                    | **LMTX †**           | **25.91** | **17.08** | **13.12** | **5.53** | **10.77** | **13.60** | **17.84** |
|                    |                      |       |       |       |        |        |        |        |
| **LF-WikiSeeAlso** | Glove                | 3.86  | 2.76  | 2.21  | 2.12   | 4.11   | 5.22   | 6.95   |
|                    | SentBERT             | 1.71  | 1.27  | 1.06  | 1.08   | 2.16   | 2.90   | 4.17   |
|                    | SimCSE               | 9.03  | 6.64  | 5.22  | 4.99   | 9.89   | 12.34  | 15.93  |
|                    | ICT                  | 10.76 | 10.05 | 8.12  | 6.12   | 14.32  | 18.05  | 23.01  |
|                    | MPNet                | 13.75 | 11.93 | 9.58  | 8.14   | 17.77  | 22.21  | 28.11  |
|                    | MACLR                | 16.31 | 13.53 | 10.78 | 9.71   | 20.39  | 25.37  | 32.05  |
|                    | MSMARCO-distilBERT   | 14.93 | 12.65 | 10.08 | 8.99   | 19.25  | 23.99  | 30.19  |
|                    | RTS                  | 18.64 | 15.14 | 12.07 | 10.86  | 22.68  | 28.29  | 35.47  |
|                    | **LMTX**             | **19.11** | **14.00** | **10.95** | **11.41** | **21.38** | **26.10** | **32.44** |
|                    |                      |       |       |       |        |        |        |        |
| **LF-Wikipedia**   | Glove                | 2.19  | 1.52  | 1.23  | 0.85   | 1.66   | 2.18   | 3.10   |
|                    | SentBERT             | 0.17  | 0.15  | 0.13  | 0.05   | 0.13   | 0.18   | 0.30   |
|                    | SimCSE               | 14.32 | 6.84  | 4.55  | 4.24   | 8.03   | 11.26  | 14.35  |
|                    | ICT                  | 17.74 | 9.67  | 7.06  | 7.35   | 11.60  | 13.84  | 17.19  |
|                    | MPNet                | 22.46 | 12.87 | 9.49  | 8.74   | 14.07  | 16.76  | 20.64  |
|                    | MSMARCO-distilBERT   | 21.62 | 12.75 | 9.52  | 8.27   | 13.81  | 16.68  | 20.89  |
|                    | MACLR                | 28.44 | 17.75 | 13.53 | 10.40  | 18.16  | 22.38  | 28.52  |
|                    | RTS                  | 30.67 | 19.03 | 14.34 | 10.58  | 18.48  | 22.51  | 28.23  |
|                    | **LMTX**             | **40.25** | **23.00** | **16.81** | **13.65** | **22.15** | **26.16** | **31.61** |


> †: 기존 최고 성능 대비 통계적으로 유의미한 향상 (paired t-test, *p* ≤ 0.01)  
> §: 기존 최고 베이스라인  
> ✅ LMTX는 모든 데이터셋에서 기존 SOTA를 상회하는 성능을 보임.

### 5. Ablation and comprehensive analysis
#### Teacher Models 평가
- 오픈소스 LLM을 다양하게 썼는데, 모델마다 잘 어울리는게 다름.
- 특정한 LLM에 종속되지 않고, 적합한 교사 모델을 선택해야 한다고함.

| Dataset                  | LLM Model   | P@1   | P@5   | R@1   | R@5   |
|--------------------------|-------------|--------|--------|--------|--------|
| AmazonCat-13K           | WizardLM    | **25.91** | **13.12** | **5.53**  | **13.60** |
|                         | Vicuna      | 25.01 | 12.70 | 5.24  | 12.95 |
|                         | Llama2      | 25.21 | 12.76 | 5.34  | 13.22 |
| LF-WikiSeeAlso-320K     | WizardLM    | **19.11** | 10.95 | **11.41** | 26.10 |
|                         | Vicuna      | 17.76 | **11.07** | 10.91 | **26.58** |
|                         | Llama2      | 17.59 | 10.64 | 10.46 | 25.27 |
| LF-Wikipedia-500K       | WizardLM    | 40.25 | 16.81 | 13.65 | 26.16 |
|                         | Vicuna      | 39.37 | 16.78 | 13.47 | 26.04 |
|                         | Llama2      | **41.67** | **17.20** | **14.37** | **26.86** |

#### Sample Size와 학습시간의 영향
![image](https://github.com/user-attachments/assets/c8f8e512-2aff-43d7-b3e7-6fbb05968910)
- 훈련 샘플이 커지면 모델 성능이 향상됨
- 그러나 이는 훈련시간의 상당한 증가를 수반하기 떄문에, 이 둘의 균형을 신중히 고려해야 함.

#### Initialization Robustness
- 사전 학습된 DistilBERT가 그냥 좋아서 잘 되는거 아님?
  -> 아니라는 걸 증명하기 위해 기존에 잘 나오던 EZ-XMC방법론이었던 RTS 방법론과 비교. LMXT가 더 좋다더라.

| Dataset              | Method     | P@1   | P@5   | R@1   | R@5   |
|----------------------|------------|--------|--------|--------|--------|
| AmazonCat-13K        | RTS-SI     | 17.87 | 10.35 | 3.57  | 10.61 |
|                      | **LMTX**   | 25.91 | 13.12 | 5.53  | 13.60 |
| WikiSeeAlso-320K     | RTS-SI     | 14.82 | 8.89  | 8.41  | 21.02 |
|                      | **LMTX**   | 19.11 | 10.95 | 11.41 | 26.10 |

#### Negative Sampling과 LLM의 Hard Negatives 비교
![image](https://github.com/user-attachments/assets/540e2356-aa25-4e38-a5b5-9007744ecc63)
- 아까 잠시 언급한, LLM이 뽑아준 'NO'인 negative label하고 (Hard negatives), 같은 배치 내, 다른 문서의 Label을 사용하는 것(In-batch)의 비교.
- In batch만 쓰는게 전반적으로 성능이 더 좋음.


### A.5 pseudo-label의 성능 비교
![image](https://github.com/user-attachments/assets/cc8eede9-7715-4806-a339-7b3b2bb00f10)
- ground truth가 있는 데이터에 대해 에포크에 따라 overlap되는 비율의 그래프.
- 에포크가 올라갈수록 성능이 올라가는 것을 알 수 있음.

#### 느낀 점
1. 지금 나에게 딱 필요한 논문임. 조만간에 구현해봐야겠음.
2. A100 2대로 대략 하루정도 걸림. 이것도 딱 우리 회사 수준임. 
3. 전체적으로 어떻게든 자원을 덜 쓰려고 온몸 비트는게 인상깊음.
4. 그치만 여전히 negative labeling은 못믿겠는걸...
